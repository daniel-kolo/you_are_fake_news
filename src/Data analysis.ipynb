{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T17:31:46.673810Z",
     "start_time": "2019-10-15T17:31:45.907651Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T17:32:08.966944Z",
     "start_time": "2019-10-15T17:32:03.808301Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"../data/reliable_news\", \"r\") as reliable_file:\n",
    "    rel = [line for line in reliable_file]\n",
    "with open(\"../data/fake_news\", \"r\") as fake_file:\n",
    "    fake = [line for line in fake_file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T17:34:43.549176Z",
     "start_time": "2019-10-15T17:32:13.282942Z"
    }
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words=stopwords.words(\"english\"),\\\n",
    "                     lowercase=True)\n",
    "word_matrix = cv.fit_transform(fake+rel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T18:25:15.733846Z",
     "start_time": "2019-10-15T18:25:15.229280Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "said: 288714\n",
      "one: 243357\n",
      "trump: 210444\n",
      "people: 204452\n",
      "new: 195937\n",
      "would: 193453\n",
      "also: 174696\n",
      "blockchain: 165557\n",
      "time: 150645\n",
      "two: 149476\n",
      "year: 148479\n",
      "like: 145384\n",
      "us: 143047\n",
      "next: 140128\n",
      "first: 136546\n",
      "com: 132750\n",
      "world: 119658\n",
      "state: 117949\n",
      "president: 116161\n",
      "many: 113357\n",
      "news: 112898\n",
      "government: 111394\n",
      "could: 108779\n",
      "even: 108600\n",
      "years: 108548\n",
      "may: 106646\n",
      "2016: 105752\n",
      "market: 104250\n",
      "get: 99621\n",
      "think: 97837\n",
      "last: 96934\n",
      "stocks: 96275\n",
      "day: 90104\n",
      "make: 89846\n",
      "headline: 86753\n",
      "bitcoin: 86663\n",
      "see: 85099\n",
      "back: 84739\n",
      "well: 84615\n",
      "source: 84509\n",
      "searches: 83527\n",
      "exceed: 83253\n",
      "way: 81917\n",
      "right: 80520\n",
      "states: 80100\n",
      "http: 78226\n",
      "know: 76665\n",
      "obama: 76211\n",
      "much: 75783\n",
      "10: 73453\n"
     ]
    }
   ],
   "source": [
    "word_frequency = word_matrix.sum(axis=0)\n",
    "inv_vocab = {v: k for k, v in cv.vocabulary_.items()}\n",
    "\n",
    "for i in range(50):\n",
    "    print(str(inv_vocab[word_frequency.argmax()])+\": \"+str(word_frequency.max()))\n",
    "    word_frequency[0, word_frequency.argmax()] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Texts' length statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T17:36:24.926176Z",
     "start_time": "2019-10-15T17:36:24.922561Z"
    }
   },
   "outputs": [],
   "source": [
    "from training_preprocess import sequence_vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T17:52:05.398134Z",
     "start_time": "2019-10-15T17:52:05.387180Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_val, X_test = fake+rel, [], []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T17:58:23.389929Z",
     "start_time": "2019-10-15T17:52:31.432805Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train, x_val, x_test, word_index = sequence_vectorize(X_train, X_val, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T18:19:48.671024Z",
     "start_time": "2019-10-15T18:19:48.622246Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34328"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(i) for i in set(x_train)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deep_learning]",
   "language": "python",
   "name": "conda-env-deep_learning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
