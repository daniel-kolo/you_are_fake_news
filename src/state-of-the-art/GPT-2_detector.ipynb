{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* the base of this modell https://github.com/openai/gpt-2-output-dataset\n",
    "* you should run this notebook from the root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the ipnut 'query' with a pytorch model, in 'device'\n",
    "def eval(query, tokenizer, model, device):\n",
    "    \n",
    "    tokens = tokenizer.encode(query)\n",
    "    all_tokens = len(tokens)\n",
    "    #RoBERTa is based on BERT so it can't handle longer sequences then 512\n",
    "    tokens = tokens[:tokenizer.max_len - 2]\n",
    "    used_tokens = len(tokens)\n",
    "    # add begin of the sentence, and end of sentenc tokens\n",
    "    tokens = torch.tensor([tokenizer.bos_token_id] + tokens + [tokenizer.eos_token_id]).unsqueeze(0)\n",
    "    mask = torch.ones_like(tokens)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tokens = tokens.to(device)\n",
    "        attention_mask=mask.to(device)\n",
    "        logits = model(tokens, attention_mask)[0]\n",
    "        probs = logits.softmax(dim=-1)\n",
    "\n",
    "    fake, real = probs.detach().cpu().flatten().numpy().tolist()\n",
    "\n",
    "    if(fake>real):\n",
    "        return(\"fake\")\n",
    "    else:\n",
    "        return(\"real\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"./models/base/detector-base.pt\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device='cuda'\n",
    "else:\n",
    "    device='cpu'\n",
    "\n",
    "# load the model \n",
    "data = torch.load(checkpoint, map_location='cpu')\n",
    "model_name = 'roberta-large' if data['args']['large'] else 'roberta-base'\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "model.load_state_dict(data['model_state_dict'])\n",
    "model.eval()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if machine is true we read the machine vs real task, else we read the fake vs real task\n",
    "\n",
    "ef read_data(machine):\n",
    "    if(machine):\n",
    "        with open(\"../test_data/gpt2_generated.txt\", \"r\") as gpt2, open(\"../test_data/grover_generated.txt\", \"r\") as grover:\n",
    "            X_test = [line for line in gpt2]+[line for line in grover]\n",
    "        \n",
    "        with open(\"../test_data/x_test.txt\") as data, open(\"../test_data/y_test.txt\") as label_file:\n",
    "            index = 0\n",
    "            labels = [label.strip() for label in label_file]\n",
    "            for i, line in enumerate(data):\n",
    "                if(index==180):\n",
    "                    break\n",
    "                if(labels[i]==\"0\"):\n",
    "                    X_test.append(line)\n",
    "                    index += 1\n",
    "        Y_test = [\"fake\" if i < 180 else \"real\" for i in range(360)]\n",
    "    else:\n",
    "        with open(\"../test_data/x_test.txt\") as f:\n",
    "            X_test = [line for line in f]\n",
    "        with open(\"../test_data/y_test.txt\") as f:\n",
    "            Y_test = [\"fake\" if line.strip() == \"1\" else \"real\" for line in f]\n",
    "    return X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, Y_test = read_data(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "result = []\n",
    "for article in X_test:\n",
    "    result.append(eval(article, tokenizer, model, device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fake       0.60      0.12      0.20      4947\n",
      "        real       0.52      0.92      0.66      5053\n",
      "\n",
      "    accuracy                           0.53     10000\n",
      "   macro avg       0.56      0.52      0.43     10000\n",
      "weighted avg       0.56      0.53      0.44     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fake vs real news\n",
    "print(classification_report(Y_test, result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fake       0.90      0.54      0.68       180\n",
      "        real       0.67      0.94      0.78       180\n",
      "\n",
      "    accuracy                           0.74       360\n",
      "   macro avg       0.79      0.74      0.73       360\n",
      "weighted avg       0.79      0.74      0.73       360\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Machine vs real news\n",
    "# print(classification_report(Y_test, result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fake       0.90      0.98      0.94       100\n",
      "        real       0.99      0.94      0.96       180\n",
      "\n",
      "    accuracy                           0.95       280\n",
      "   macro avg       0.94      0.96      0.95       280\n",
      "weighted avg       0.96      0.95      0.95       280\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Only GPT-2 generated, and real news\n",
    "#print(classification_report(Y_test[:100]+Y_test[180:], result))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deep_learning]",
   "language": "python",
   "name": "conda-env-deep_learning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
