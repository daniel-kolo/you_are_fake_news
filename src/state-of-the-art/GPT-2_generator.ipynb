{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The base of this code is from https://github.com/openai/gpt-2\n",
    "* you should clone the repository and run this notebook from /src folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the model with 774 million weight\n",
    "!python ../download_model.py 774M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fire\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import model, sample, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interact_model(\n",
    "    querys,\n",
    "    model_name='774M',\n",
    "    seed=None,\n",
    "    nsamples=1,\n",
    "    batch_size=1,\n",
    "    length=None,\n",
    "    temperature=1,\n",
    "    top_k=0,\n",
    "    top_p=1,\n",
    "    models_dir='../models',\n",
    "):\n",
    "    \"\"\"\n",
    "    Interactively run the model\n",
    "    :model_name=124M : String, which model to use\n",
    "    :seed=None : Integer seed for random number generators, fix seed to reproduce\n",
    "     results\n",
    "    :nsamples=1 : Number of samples to return total\n",
    "    :batch_size=1 : Number of batches (only affects speed/memory).  Must divide nsamples.\n",
    "    :length=None : Number of tokens in generated text, if None (default), is\n",
    "     determined by model hyperparameters\n",
    "    :temperature=1 : Float value controlling randomness in boltzmann\n",
    "     distribution. Lower temperature results in less random completions. As the\n",
    "     temperature approaches zero, the model will become deterministic and\n",
    "     repetitive. Higher temperature results in more random completions.\n",
    "    :top_k=0 : Integer value controlling diversity. 1 means only 1 word is\n",
    "     considered for each step (token), resulting in deterministic completions,\n",
    "     while 40 means 40 words are considered at each step. 0 (default) is a\n",
    "     special setting meaning no restrictions. 40 generally is a good value.\n",
    "     :models_dir : path to parent folder containing model subfolders\n",
    "     (i.e. contains the <model_name> folder)\n",
    "    \"\"\"\n",
    "    models_dir = os.path.expanduser(os.path.expandvars(models_dir))\n",
    "    if batch_size is None:\n",
    "        batch_size = 1\n",
    "    assert nsamples % batch_size == 0\n",
    "\n",
    "    enc = encoder.get_encoder(model_name, models_dir)\n",
    "    hparams = model.default_hparams()\n",
    "    with open(os.path.join(models_dir, model_name, 'hparams.json')) as f:\n",
    "        hparams.override_from_dict(json.load(f))\n",
    "\n",
    "    if length is None:\n",
    "        length = hparams.n_ctx // 2\n",
    "    elif length > hparams.n_ctx:\n",
    "        raise ValueError(\"Can't get samples longer than window size: %s\" % hparams.n_ctx)\n",
    "\n",
    "    with tf.Session(graph=tf.Graph()) as sess:\n",
    "        context = tf.placeholder(tf.int32, [batch_size, None])\n",
    "        np.random.seed(seed)\n",
    "        tf.set_random_seed(seed)\n",
    "        output = sample.sample_sequence(\n",
    "            hparams=hparams, length=length,\n",
    "            context=context,\n",
    "            batch_size=batch_size,\n",
    "            temperature=temperature, top_k=top_k, top_p=top_p\n",
    "        )\n",
    "\n",
    "        saver = tf.train.Saver()\n",
    "        ckpt = tf.train.latest_checkpoint(os.path.join(models_dir, model_name))\n",
    "        saver.restore(sess, ckpt)\n",
    "        texts = []\n",
    "        for query in querys:\n",
    "            context_tokens = enc.encode(query)\n",
    "            generated = 0\n",
    "            for _ in range(nsamples // batch_size):\n",
    "                out = sess.run(output, feed_dict={context: [context_tokens for _ in range(batch_size)]}\n",
    "                              )[:, len(context_tokens):]\n",
    "                for i in range(batch_size):\n",
    "                    generated += 1\n",
    "                    text = enc.decode(out[i])\n",
    "                    texts.append(text)\n",
    "        return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "querys = [\"Poland could have to leave the EU over its judicial reform proposals, the country's Supreme Court has warned.\",\n",
    "        \"This is the face of a woman who lived 6,000 years ago in Scandinavia.\",\n",
    "        \"The Trump administration has said it does not consider the mass killings of Armenians in 1915 to be a genocide, contradicting a unanimous vote by the US Senate.\",\n",
    "        \"Toronto's Danforth Avenue shooting victims have launched a class action lawsuit against US gun maker Smith & Wesson.\",\n",
    "        \"A man who built an exploding glitter bomb last Christmas which went viral online has added stronger smells and a police soundtrack to his latest version.\",\n",
    "        \"A gin company has been ordered to pay Dame Vera Lynn Â£1,800 in legal costs after losing a case to trademark the singer's name for its drink.\",\n",
    "        \"A woman in southern Germany has taken a cheese shop to court over her right to display signs complaining about the smell.\",\n",
    "        \"Supreme Court Justice Ruth Bader Ginsburg has responded to Donald Trump's call for the top US court to stop impeachment.\",\n",
    "        \"President Donald Trump has lashed out over his impending impeachment in an irate letter to top Democrat Nancy Pelosi, accusing her of declaring 'open war on American democracy'.\",\n",
    "        \"The Pope has declared that the rule of 'pontifical secrecy' no longer applies to the sexual abuse of minors, in a bid to improve transparency in such cases.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before starting09:19:08\n",
      "WARNING:tensorflow:From /home/miniconda3/envs/deep_learning/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/work/deep_learning/gpt-2/src/sample.py:64: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/work/deep_learning/gpt-2/src/sample.py:67: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.random.categorical instead.\n",
      "WARNING:tensorflow:From /home/miniconda3/envs/deep_learning/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from ../models/774M/model.ckpt\n",
      "0. loop: 09:24:54\n",
      "0. loop: 09:24:54\n",
      "0. loop: 09:24:54\n",
      "0. loop: 09:24:54\n",
      "0. loop: 09:24:54\n",
      "0. loop: 09:24:54\n",
      "0. loop: 09:24:54\n",
      "0. loop: 09:24:54\n",
      "0. loop: 09:24:54\n",
      "0. loop: 09:24:54\n",
      "INFO:tensorflow:Restoring parameters from ../models/774M/model.ckpt\n",
      "1. loop: 09:30:12\n",
      "1. loop: 09:30:12\n",
      "1. loop: 09:30:12\n",
      "1. loop: 09:30:12\n",
      "1. loop: 09:30:12\n",
      "1. loop: 09:30:12\n",
      "1. loop: 09:30:12\n",
      "1. loop: 09:30:12\n",
      "1. loop: 09:30:12\n",
      "1. loop: 09:30:12\n",
      "INFO:tensorflow:Restoring parameters from ../models/774M/model.ckpt\n",
      "2. loop: 09:35:30\n",
      "2. loop: 09:35:30\n",
      "2. loop: 09:35:30\n",
      "2. loop: 09:35:30\n",
      "2. loop: 09:35:30\n",
      "2. loop: 09:35:30\n",
      "2. loop: 09:35:30\n",
      "2. loop: 09:35:31\n",
      "2. loop: 09:35:31\n",
      "2. loop: 09:35:31\n",
      "INFO:tensorflow:Restoring parameters from ../models/774M/model.ckpt\n",
      "3. loop: 09:40:47\n",
      "3. loop: 09:40:47\n",
      "3. loop: 09:40:47\n",
      "3. loop: 09:40:47\n",
      "3. loop: 09:40:47\n",
      "3. loop: 09:40:47\n",
      "3. loop: 09:40:47\n",
      "3. loop: 09:40:47\n",
      "3. loop: 09:40:47\n",
      "3. loop: 09:40:47\n",
      "INFO:tensorflow:Restoring parameters from ../models/774M/model.ckpt\n",
      "4. loop: 09:46:05\n",
      "4. loop: 09:46:05\n",
      "4. loop: 09:46:05\n",
      "4. loop: 09:46:05\n",
      "4. loop: 09:46:05\n",
      "4. loop: 09:46:05\n",
      "4. loop: 09:46:05\n",
      "4. loop: 09:46:05\n",
      "4. loop: 09:46:05\n",
      "4. loop: 09:46:05\n",
      "INFO:tensorflow:Restoring parameters from ../models/774M/model.ckpt\n",
      "5. loop: 09:51:24\n",
      "5. loop: 09:51:24\n",
      "5. loop: 09:51:24\n",
      "5. loop: 09:51:24\n",
      "5. loop: 09:51:24\n",
      "5. loop: 09:51:24\n",
      "5. loop: 09:51:24\n",
      "5. loop: 09:51:24\n",
      "5. loop: 09:51:24\n",
      "5. loop: 09:51:24\n",
      "INFO:tensorflow:Restoring parameters from ../models/774M/model.ckpt\n",
      "6. loop: 09:56:41\n",
      "6. loop: 09:56:41\n",
      "6. loop: 09:56:41\n",
      "6. loop: 09:56:41\n",
      "6. loop: 09:56:41\n",
      "6. loop: 09:56:41\n",
      "6. loop: 09:56:41\n",
      "6. loop: 09:56:41\n",
      "6. loop: 09:56:41\n",
      "6. loop: 09:56:41\n",
      "INFO:tensorflow:Restoring parameters from ../models/774M/model.ckpt\n",
      "7. loop: 10:01:59\n",
      "7. loop: 10:01:59\n",
      "7. loop: 10:01:59\n",
      "7. loop: 10:01:59\n",
      "7. loop: 10:01:59\n",
      "7. loop: 10:01:59\n",
      "7. loop: 10:01:59\n",
      "7. loop: 10:01:59\n",
      "7. loop: 10:01:59\n",
      "7. loop: 10:01:59\n",
      "INFO:tensorflow:Restoring parameters from ../models/774M/model.ckpt\n",
      "8. loop: 10:07:16\n",
      "8. loop: 10:07:16\n",
      "8. loop: 10:07:16\n",
      "8. loop: 10:07:16\n",
      "8. loop: 10:07:16\n",
      "8. loop: 10:07:16\n",
      "8. loop: 10:07:16\n",
      "8. loop: 10:07:16\n",
      "8. loop: 10:07:16\n",
      "8. loop: 10:07:16\n",
      "INFO:tensorflow:Restoring parameters from ../models/774M/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "# generate 100 text, with the seeds above\n",
    "print('before starting' + strftime(\"%H:%M:%S\", localtime()))\n",
    "\n",
    "with open(\"../generated_texts.txt\", \"a+\") as f:\n",
    "    for i in range(10):\n",
    "        for text in interact_model(querys, model_name=\"774M\"):\n",
    "            f.write(text.replace(\"\\n\", \" \")+\"\\n\")\n",
    "            print(str(i)+'. loop: '+ strftime(\"%H:%M:%S\", localtime()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deep_learning]",
   "language": "python",
   "name": "conda-env-deep_learning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
