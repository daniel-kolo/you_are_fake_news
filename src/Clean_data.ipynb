{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter \n",
    "\n",
    "def get_most_common_n(data_set, num):\n",
    "    counter = Counter(data_set) \n",
    "    most_occur = counter.most_common(num)\n",
    "    return most_occur\n",
    "\n",
    "def chunkIt(seq, num):\n",
    "    avg = len(seq) / float(num)\n",
    "    out = []\n",
    "    last = 0.0\n",
    "\n",
    "    while last < len(seq):\n",
    "        out.append(seq[int(last):int(last + avg)])\n",
    "        last += avg\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------\n",
    "# Only use this if you have the non-chunked version of the data on your host\n",
    "#---------------------------------------------------------------------------\n",
    "with open('data/fake_news_prep', 'r') as file:\n",
    "    fake_corpus = file.read().replace('\\n', '')\n",
    "    \n",
    "with open('data/reliable_news_prep', 'r') as file:\n",
    "    rel_corpus = file.read().replace('\\n', '')\n",
    "\n",
    "    \n",
    "fake_corpus = fake_corpus.split()\n",
    "rel_corpus = rel_corpus.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------\n",
    "# Use this if you have the chunked version of the data on your host\n",
    "#------------------------------------------------------------------\n",
    "\n",
    "with open('data/fake1', 'r') as file:\n",
    "    fake1 = file.read().replace('\\n', ' ')\n",
    "    \n",
    "with open('data/fake2', 'r') as file:\n",
    "    fake2 = file.read().replace('\\n', ' ')\n",
    "    \n",
    "with open('data/fake3', 'r') as file:\n",
    "    fake3 = file.read().replace('\\n', ' ')\n",
    "    \n",
    "with open('data/rel1', 'r') as file:\n",
    "    rel1 = file.read().replace('\\n', ' ')\n",
    "    \n",
    "with open('data/rel2', 'r') as file:\n",
    "    rel2 = file.read().replace('\\n', ' ')\n",
    "    \n",
    "with open('data/rel3', 'r') as file:\n",
    "    rel3 = file.read().replace('\\n', ' ')\n",
    "\n",
    "fake1 = fake1.split()\n",
    "fake2 = fake2.split()\n",
    "fake3 = fake3.split()\n",
    "\n",
    "rel1 = rel1.split()\n",
    "rel2 = rel2.split()\n",
    "rel3 = rel3.split()\n",
    "\n",
    "fake_corpus = fake1 + fake2 + fake3\n",
    "rel_corpus = rel1 + rel2 + rel3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Serialize data to Github compatible sized chunks\n",
    "fake1, fake2 , fake3 = chunkIt(fake_corpus, 3)\n",
    "rel1, rel2, rel3 = chunkIt(rel_corpus, 3)\n",
    "\n",
    "with open('data/fake1', 'w') as filehandle:\n",
    "    for listitem in fake1:\n",
    "        filehandle.write('%s\\n' % listitem)\n",
    "        \n",
    "with open('data/fake2', 'w') as filehandle:\n",
    "    for listitem in fake2:\n",
    "        filehandle.write('%s\\n' % listitem)\n",
    "        \n",
    "with open('data/fake3', 'w') as filehandle:\n",
    "    for listitem in fake3:\n",
    "        filehandle.write('%s\\n' % listitem)\n",
    "        \n",
    "with open('data/rel1', 'w') as filehandle:\n",
    "    for listitem in rel1:\n",
    "        filehandle.write('%s\\n' % listitem)\n",
    "        \n",
    "with open('data/rel2', 'w') as filehandle:\n",
    "    for listitem in rel2:\n",
    "        filehandle.write('%s\\n' % listitem)\n",
    "        \n",
    "with open('data/rel3', 'w') as filehandle:\n",
    "    for listitem in rel3:\n",
    "        filehandle.write('%s\\n' % listitem)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get most common words in each corpus, write them to file \n",
    "most_common_fake = get_most_common_n(fake_corpus, 2000)\n",
    "most_common_reliable = get_most_common_n(rel_corpus, 2000)\n",
    "\n",
    "with open('data/most_common_words_fake', 'w') as filehandle:\n",
    "    for listitem in most_common_fake:\n",
    "        filehandle.write('{} {}\\n'.format(listitem[0],listitem[1]))\n",
    "        \n",
    "with open('data/most_common_words_reliable', 'w') as filehandle:\n",
    "    for listitem in most_common_reliable:\n",
    "        filehandle.write('{} {}\\n'.format(listitem[0],listitem[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating two lists that contian words that are contained by only one of the corpuses\n",
    "fake_list = []\n",
    "rel_list = []\n",
    "\n",
    "for i in range(len(most_common_fake)):\n",
    "    fake_list.append(most_common_fake[i][0])\n",
    "    \n",
    "for i in range(len(most_common_reliable)):\n",
    "    rel_list.append(most_common_reliable[i][0])\n",
    "\n",
    "del_list1 = [item for item in rel_list if item in fake_list]\n",
    "del_list2 = [item for item in fake_list if item in rel_list]\n",
    "\n",
    "final_fake = [item for item in fake_list if (item not in del_list1 or item not in del_list2)]\n",
    "final_rel = [item for item in rel_list if (item not in del_list1 or item not in del_list2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/unique_most_common_words_reliable', 'w') as filehandle:\n",
    "    for listitem in final_fake:\n",
    "        filehandle.write('{}\\n'.format(listitem))\n",
    "        \n",
    "with open('data/unique_most_common_words_fake', 'w') as filehandle:\n",
    "    for listitem in final_rel:\n",
    "        filehandle.write('{}\\n'.format(listitem))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
