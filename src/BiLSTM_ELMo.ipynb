{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* this notebook is very similar to the 'BiLSTM network' notebook, the major difference is the embedding layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T17:19:49.981159Z",
     "start_time": "2019-11-05T17:19:47.311093Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"../data/reliable_news_prep\", \"r\") as reliable_file:\n",
    "    rel = [line.strip() for line in reliable_file]\n",
    "with open(\"../data/fake_news_prep\", \"r\") as fake_file:\n",
    "    fake = [line.strip() for line in fake_file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T17:20:18.620348Z",
     "start_time": "2019-11-05T17:20:18.616492Z"
    }
   },
   "outputs": [],
   "source": [
    "from training_preprocess import prepare_fastText_embedding_matrix as prepare_embedding\n",
    "from training_preprocess import sequence_vectorize\n",
    "from training_preprocess import train_val_test_split as split\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T17:29:22.054619Z",
     "start_time": "2019-11-05T17:29:22.048775Z"
    }
   },
   "outputs": [],
   "source": [
    "def label_to_vector(labels):\n",
    "    array = np.zeros([len(labels), 2])\n",
    "    for i, label in enumerate(labels):\n",
    "        array[i, label] = 1\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T17:20:20.392530Z",
     "start_time": "2019-11-05T17:20:20.212050Z"
    }
   },
   "outputs": [],
   "source": [
    "length=1000\n",
    "\n",
    "# we decreased the sentence length because the ELMo embedding used to much memory, \n",
    "max_sentence_len=512\n",
    "\n",
    "text = rel[:length]+fake[:length]\n",
    "labels = [0 if i<length else 1 for i in range(2*length)] # reliable - 0; fake - 1\n",
    "\n",
    "X_train, X_val, X_test, Y_train, Y_test, Y_val = split(text, labels, 0.2, 0.1)\n",
    "X_train = [' '.join(t.split()[:max_sentence_len]) for t in X_train]\n",
    "X_val = [' '.join(t.split()[:max_sentence_len]) for t in X_val]\n",
    "\n",
    "x_train = np.array(X_train, dtype=object)[:, np.newaxis]\n",
    "x_val = np.array(X_val, dtype=object)[:, np.newaxis]\n",
    "\n",
    "y_train = label_to_vector(Y_train)\n",
    "y_val = label_to_vector(Y_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T17:29:22.150949Z",
     "start_time": "2019-11-05T17:29:22.056738Z"
    }
   },
   "outputs": [],
   "source": [
    "def bidirectional_LSTM(dropout_rate):\n",
    "    \n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((1, ), dtype=\"string\")\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = ElmoEmbeddingLayer()(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(dropout_rate)(embedding_layer)\n",
    "    \n",
    "    # Add the LSTM Layer\n",
    "    lstm_layer = layers.Bidirectional(layers.LSTM(20))(embedding_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(70, activation=\"relu\")(lstm_layer)\n",
    "    output_layer2 = layers.Dropout(dropout_rate)(output_layer1)\n",
    "    output_layer3 = layers.Dense(2, activation=\"softmax\")(output_layer2)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer3)\n",
    "    model.compile(optimizer='Adam', loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "from keras import layers, models, optimizers, regularizers\n",
    "from keras.engine import Layer\n",
    "\n",
    "#this code is based on https://github.com/strongio/keras-elmo/blob/master/Elmo%20Keras.ipynb this repository\n",
    "class ElmoEmbeddingLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.dimensions = 1024\n",
    "        self.trainable=True\n",
    "        super(ElmoEmbeddingLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.elmo = hub.Module('https://tfhub.dev/google/elmo/2', trainable=self.trainable,\n",
    "                               name=\"{}_module\".format(self.name))\n",
    "\n",
    "        self.trainable_weights += K.tf.trainable_variables(scope=\"^{}_module/.*\".format(self.name))\n",
    "        super(ElmoEmbeddingLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        result = self.elmo(K.squeeze(K.cast(x, tf.string), axis=1),\n",
    "                      as_dict=True,\n",
    "                      signature='default',\n",
    "                      )['elmo']\n",
    "        return result\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return K.not_equal(inputs, '--PAD--')\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], 48, self.dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/miniconda3/envs/deep_learning/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/miniconda3/envs/deep_learning/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/miniconda3/envs/deep_learning/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/miniconda3/envs/deep_learning/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "elmo_embedding_layer_1 (Elmo (None, 48, 1024)          4         \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 48, 1024)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 40)                167200    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 70)                2870      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 70)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 142       \n",
      "=================================================================\n",
      "Total params: 170,216\n",
      "Trainable params: 170,216\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = bidirectional_LSTM(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T23:40:09.826284Z",
     "start_time": "2019-11-05T17:29:22.383843Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1400 samples, validate on 400 samples\n",
      "Epoch 1/10\n",
      " - 243s - loss: 0.5280 - acc: 0.7321 - val_loss: 0.3710 - val_acc: 0.8275\n",
      "Epoch 2/10\n",
      " - 238s - loss: 0.3362 - acc: 0.8593 - val_loss: 0.3263 - val_acc: 0.8575\n",
      "Epoch 3/10\n",
      " - 238s - loss: 0.2711 - acc: 0.8957 - val_loss: 0.2748 - val_acc: 0.8650\n",
      "Epoch 4/10\n",
      " - 239s - loss: 0.2149 - acc: 0.9121 - val_loss: 0.2766 - val_acc: 0.8825\n",
      "Epoch 5/10\n",
      " - 238s - loss: 0.1938 - acc: 0.9236 - val_loss: 0.2445 - val_acc: 0.9050\n",
      "Epoch 6/10\n",
      " - 238s - loss: 0.1675 - acc: 0.9350 - val_loss: 0.2594 - val_acc: 0.8875\n",
      "Epoch 7/10\n",
      " - 238s - loss: 0.1745 - acc: 0.9307 - val_loss: 0.2481 - val_acc: 0.8950\n",
      "Epoch 8/10\n",
      " - 238s - loss: 0.1563 - acc: 0.9371 - val_loss: 0.2565 - val_acc: 0.8850\n"
     ]
    }
   ],
   "source": [
    "from keras import callbacks\n",
    "\n",
    "callbacks = [callbacks.EarlyStopping(monitor='val_loss', patience=3)]\n",
    "\n",
    "model.fit(x_train,\n",
    "          y_train,\n",
    "          epochs=10,\n",
    "          callbacks=callbacks,\n",
    "          validation_data=(x_val, y_val),\n",
    "          verbose=2,  # Logs once per epoch.\n",
    "          batch_size=16)\n",
    "\n",
    "model.save_weights('./BiLSTM_ELMo.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if machine is true we read the machine vs real task, else we read the fake vs real task\n",
    "\n",
    "def read_data(machine):\n",
    "    if(machine):\n",
    "        with open(\"../../test_data/gpt2_generated.txt\", \"r\") as gpt2, open(\"../../test_data/grover_generated.txt\", \"r\") as grover:\n",
    "            X_test = [line for line in gpt2]+[line for line in grover]\n",
    "        \n",
    "        with open(\"../../test_data/x_test.txt\") as data, open(\"../../test_data/y_test.txt\") as label_file:\n",
    "            index = 0\n",
    "            labels = [label.strip() for label in label_file]\n",
    "            for i, line in enumerate(data):\n",
    "                if(index==180):\n",
    "                    break\n",
    "                if(labels[i]==\"0\"):\n",
    "                    X_test.append(line)\n",
    "                    index += 1\n",
    "        Y_test = [\"fake\" if i < 180 else \"real\" for i in range(360)]\n",
    "    else:\n",
    "        with open(\"../../test_data/x_test.txt\") as f:\n",
    "            X_test = [line for line in f]\n",
    "        with open(\"../../test_data/y_test.txt\") as f:\n",
    "            Y_test = [\"fake\" if line.strip() == \"1\" else \"real\" for line in f]\n",
    "    return X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, Y_test = read_data(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sentence_len = 512\n",
    "X_test = [' '.join(t.split()[:max_sentence_len]) for t in X_test]\n",
    "x_test = np.array(X_test, dtype=object)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_preprocess import en_lemmatize, filter_stopwords, filter_punctuation, filter_urls\n",
    "\n",
    "def predict(model, datas):\n",
    "    probs = model.predict(datas)\n",
    "    result = []\n",
    "    for i in probs:\n",
    "        if np.argmax(i)==1:\n",
    "            result.append(\"fake\")\n",
    "        else:\n",
    "            result.append(\"real\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-06T09:11:43.400862Z",
     "start_time": "2019-11-06T09:04:10.742260Z"
    }
   },
   "outputs": [],
   "source": [
    "model.load_weights('./BiLSTM_ELMo.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = predict(model, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fake       0.53      0.48      0.50       180\n",
      "        real       0.53      0.58      0.55       180\n",
      "\n",
      "    accuracy                           0.53       360\n",
      "   macro avg       0.53      0.53      0.53       360\n",
      "weighted avg       0.53      0.53      0.53       360\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# machine vs real\n",
    "# print(classification_report(Y_test, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fake       0.71      0.88      0.79      4947\n",
      "        real       0.85      0.65      0.74      5053\n",
      "\n",
      "    accuracy                           0.76     10000\n",
      "   macro avg       0.78      0.76      0.76     10000\n",
      "weighted avg       0.78      0.76      0.76     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fake vs real\n",
    "print(classification_report(Y_test, prediction))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deep_learning]",
   "language": "python",
   "name": "conda-env-deep_learning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
