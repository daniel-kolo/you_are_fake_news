{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hyper_param_optimalization_final_training.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "em8t1dIWh1Jc",
        "colab_type": "code",
        "outputId": "94a41533-588a-44db-c796-2834e1bcff79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "#Importing the git repo, for code reuse\n",
        "!git clone https://github.com/daniel-kolo/you_are_fake_news.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'you_are_fake_news'...\n",
            "remote: Enumerating objects: 54, done.\u001b[K\n",
            "remote: Counting objects:   1% (1/54)\u001b[K\rremote: Counting objects:   3% (2/54)\u001b[K\rremote: Counting objects:   5% (3/54)\u001b[K\rremote: Counting objects:   7% (4/54)\u001b[K\rremote: Counting objects:   9% (5/54)\u001b[K\rremote: Counting objects:  11% (6/54)\u001b[K\rremote: Counting objects:  12% (7/54)\u001b[K\rremote: Counting objects:  14% (8/54)\u001b[K\rremote: Counting objects:  16% (9/54)\u001b[K\rremote: Counting objects:  18% (10/54)\u001b[K\rremote: Counting objects:  20% (11/54)\u001b[K\rremote: Counting objects:  22% (12/54)\u001b[K\rremote: Counting objects:  24% (13/54)\u001b[K\rremote: Counting objects:  25% (14/54)\u001b[K\rremote: Counting objects:  27% (15/54)\u001b[K\rremote: Counting objects:  29% (16/54)\u001b[K\rremote: Counting objects:  31% (17/54)\u001b[K\rremote: Counting objects:  33% (18/54)\u001b[K\rremote: Counting objects:  35% (19/54)\u001b[K\rremote: Counting objects:  37% (20/54)\u001b[K\rremote: Counting objects:  38% (21/54)\u001b[K\rremote: Counting objects:  40% (22/54)\u001b[K\rremote: Counting objects:  42% (23/54)\u001b[K\rremote: Counting objects:  44% (24/54)\u001b[K\rremote: Counting objects:  46% (25/54)\u001b[K\rremote: Counting objects:  48% (26/54)\u001b[K\rremote: Counting objects:  50% (27/54)\u001b[K\rremote: Counting objects:  51% (28/54)\u001b[K\rremote: Counting objects:  53% (29/54)\u001b[K\rremote: Counting objects:  55% (30/54)\u001b[K\rremote: Counting objects:  57% (31/54)\u001b[K\rremote: Counting objects:  59% (32/54)\u001b[K\rremote: Counting objects:  61% (33/54)\u001b[K\rremote: Counting objects:  62% (34/54)\u001b[K\rremote: Counting objects:  64% (35/54)\u001b[K\rremote: Counting objects:  66% (36/54)\u001b[K\rremote: Counting objects:  68% (37/54)\u001b[K\rremote: Counting objects:  70% (38/54)\u001b[K\rremote: Counting objects:  72% (39/54)\u001b[K\rremote: Counting objects:  74% (40/54)\u001b[K\rremote: Counting objects:  75% (41/54)\u001b[K\rremote: Counting objects:  77% (42/54)\u001b[K\rremote: Counting objects:  79% (43/54)\u001b[K\rremote: Counting objects:  81% (44/54)\u001b[K\rremote: Counting objects:  83% (45/54)\u001b[K\rremote: Counting objects:  85% (46/54)\u001b[K\rremote: Counting objects:  87% (47/54)\u001b[K\rremote: Counting objects:  88% (48/54)\u001b[K\rremote: Counting objects:  90% (49/54)\u001b[K\rremote: Counting objects:  92% (50/54)\u001b[K\rremote: Counting objects:  94% (51/54)\u001b[K\rremote: Counting objects:  96% (52/54)\u001b[K\rremote: Counting objects:  98% (53/54)\u001b[K\rremote: Counting objects: 100% (54/54)\u001b[K\rremote: Counting objects: 100% (54/54), done.\u001b[K\n",
            "remote: Compressing objects: 100% (43/43), done.\u001b[K\n",
            "remote: Total 174 (delta 23), reused 39 (delta 11), pack-reused 120\u001b[K\n",
            "Receiving objects: 100% (174/174), 142.23 MiB | 23.47 MiB/s, done.\n",
            "Resolving deltas: 100% (51/51), done.\n",
            "Checking out files: 100% (24/24), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wG8zl97Hh5Pj",
        "colab_type": "code",
        "outputId": "440a1607-881e-4d59-ab5e-4d9d27457ccc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "# Mounting the google drive, where we've kept the downloaded embeddings, and the unsharded data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8MWJvWJkKpr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Opening and loading the data\n",
        "with open(\"../../gdrive/My Drive/reliable_news_prep\", \"r\") as reliable_file:\n",
        "    rel = [line.strip() for line in reliable_file]\n",
        "with open(\"../../gdrive/My Drive/fake_news_prep\", \"r\") as fake_file:\n",
        "    fake = [line.strip() for line in fake_file]\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c22OlNUsoPBl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Cleaning the data from the headline-s, it could be the source of the suprinsingly high accuracy values\n",
        "clean_fake = []\n",
        "for line in fake:\n",
        "    if (\"blockchain\" in line) and (\"bitcoin\" in line) and (\"headline\" in line):\n",
        "        line = line.split()\n",
        "        line = line[10:]\n",
        "        line = (\" \").join(line)\n",
        "    clean_fake.append(line)\n",
        "    \n",
        "clean_fake[0]\n",
        "fake = clean_fake"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyARFOVBoX8q",
        "colab_type": "code",
        "outputId": "c43f0535-e301-4439-ed04-f46cb961a230",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Setting the training length\n",
        "length=50000\n",
        "\n",
        "text = rel[:length]+fake[:length]\n",
        "labels = [0 if i<length else 1 for i in range(2*length)] # reliable - 0; fake - 1\n",
        "\n",
        "from training_preprocess import prepare_fastText_embedding_matrix as prepare_embedding\n",
        "from training_preprocess import sequence_vectorize\n",
        "from training_preprocess import train_val_test_split as split\n",
        "\n",
        "# Train, Test, Valid dataset creation\n",
        "X_train, X_val, X_test, Y_train, Y_test, Y_val = split(text, labels, 0.2, 0.1)\n",
        "x_train, x_val, x_test, word_index = sequence_vectorize(X_train, X_val, X_test, sequence_length=1100)\n",
        "\n",
        "#preparing the embedding matrix\n",
        "embedding_matrix = prepare_fastText_embedding_matrix(word_index, \"../../gdrive/My Drive/cc.en.300.bin\")\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  app.launch_new_instance()\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yThUFKZXubmc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from gensim.models import FastText, fasttext\n",
        "from tensorflow.python.keras.preprocessing import sequence\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Had to redefine the function, because Google colab couldn't use gensim.models\n",
        "# Couldn't figure out why, had to use a different vector loading method\n",
        "\n",
        "def prepare_fastText_embedding_matrix(word_index, model_file):\n",
        "    #model = gensim.models.fasttext.load_facebook_model(model_file)\n",
        "    model=FastText.load_fasttext_format(model_file)\n",
        "    EMBEDDING_DIM = model.vector_size\n",
        "    embedding_matrix = np.zeros((len(word_index)+1, EMBEDDING_DIM))\n",
        "    for word, i in word_index.items():\n",
        "        embedding_vector = model[word]\n",
        "        embedding_matrix[i-1] = embedding_vector\n",
        "    \n",
        "    return embedding_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNkaUzj5woCJ",
        "colab_type": "code",
        "outputId": "3757b6d4-1445-4459-b27d-7ecab981b4a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras import layers, models, optimizers, regularizers\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSD93Mibxfmy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Vectorizing the Fake News - Reliable News labels\n",
        "def label_to_vector(labels):\n",
        "    array = np.zeros([len(labels), 2])\n",
        "    for i, label in enumerate(labels):\n",
        "        array[i, label] = 1\n",
        "    return array"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ntPAK5hkwK9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#Defining neural net\n",
        "def bidirectional_LSTM(embedding_dim, dropout_rate, input_shape, num_classes, is_embedding_trainable, embedding_matrix):\n",
        "    \n",
        "    # Add an Input Layer\n",
        "    input_layer = layers.Input((input_shape, ))\n",
        "\n",
        "    # Add the word embedding Layer\n",
        "    embedding_layer = layers.Embedding(input_dim=embedding_matrix.shape[0],\n",
        "                                       output_dim=embedding_dim,\n",
        "                                       weights=[embedding_matrix],\n",
        "                                       trainable=is_embedding_trainable)(input_layer)\n",
        "    embedding_layer = layers.SpatialDropout1D(0.6)(embedding_layer)\n",
        "    \n",
        "    # Add the LSTM Layer\n",
        "    lstm_layer = layers.Bidirectional(layers.LSTM(20))(embedding_layer)\n",
        "\n",
        "    # Add the output Layers\n",
        "    output_layer1 = layers.Dense(70, activation=\"relu\")(lstm_layer)\n",
        "    output_layer2 = layers.Dropout(dropout_rate)(output_layer1)\n",
        "    output_layer3 = layers.Dense(num_classes, activation=\"softmax\")(output_layer2)\n",
        "\n",
        "    # Compile the model\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer3)\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bz4nzqqjkytv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train = label_to_vector(Y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-_G3V-pk0cT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_val = label_to_vector(Y_val)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_eqPtcyk13z",
        "colab_type": "code",
        "outputId": "073f7724-d1a1-40f8-e1b4-7fd0e2deb9eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "# First suspiciously high accuracy rates\n",
        "import keras\n",
        "\n",
        "model = bidirectional_LSTM(300, 0.2, 1100, 2, True, embedding_matrix)\n",
        "model.compile(optimizer='Adam', loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
        "\n",
        "callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)]\n",
        "\n",
        "model.fit(x_train,\n",
        "          y_train,\n",
        "          epochs=10,\n",
        "          callbacks=callbacks,\n",
        "          validation_data=(x_val, y_val),\n",
        "          verbose=2,  # Logs once per epoch.\n",
        "          batch_size=128)\n",
        "\n",
        "model.save_weights('../../gdrive/My Drive/sequence_model_with_pre_trained_embedding.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "Train on 70000 samples, validate on 20000 samples\n",
            "Epoch 1/10\n",
            " - 2874s - loss: 0.2253 - acc: 0.9083 - val_loss: 0.1916 - val_acc: 0.9223\n",
            "Epoch 2/10\n",
            " - 2875s - loss: 0.1335 - acc: 0.9508 - val_loss: 0.1415 - val_acc: 0.9498\n",
            "Epoch 3/10\n",
            " - 2849s - loss: 0.1127 - acc: 0.9582 - val_loss: 0.1439 - val_acc: 0.9513\n",
            "Epoch 4/10\n",
            " - 2854s - loss: 0.1004 - acc: 0.9629 - val_loss: 0.1351 - val_acc: 0.9525\n",
            "Epoch 5/10\n",
            " - 2838s - loss: 0.0804 - acc: 0.9702 - val_loss: 0.1470 - val_acc: 0.9487\n",
            "Epoch 6/10\n",
            " - 2923s - loss: 0.0613 - acc: 0.9784 - val_loss: 0.1279 - val_acc: 0.9617\n",
            "Epoch 7/10\n",
            " - 2938s - loss: 0.0470 - acc: 0.9835 - val_loss: 0.1265 - val_acc: 0.9634\n",
            "Epoch 8/10\n",
            " - 2943s - loss: 0.0428 - acc: 0.9850 - val_loss: 0.1447 - val_acc: 0.9616\n",
            "Epoch 9/10\n",
            " - 2894s - loss: 0.0365 - acc: 0.9872 - val_loss: 0.1484 - val_acc: 0.9660\n",
            "Epoch 10/10\n",
            " - 2854s - loss: 0.0260 - acc: 0.9908 - val_loss: 0.1620 - val_acc: 0.9650\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiGQSuyusaUi",
        "colab_type": "text"
      },
      "source": [
        "#Parameter optimalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ryyvDupskug",
        "colab_type": "code",
        "outputId": "ef87b446-cb05-4977-c10f-3dd22f3adccf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "!git clone https://github.com/daniel-kolo/you_are_fake_news.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'you_are_fake_news'...\n",
            "remote: Enumerating objects: 51, done.\u001b[K\n",
            "remote: Counting objects: 100% (51/51), done.\u001b[K\n",
            "remote: Compressing objects: 100% (40/40), done.\u001b[K\n",
            "remote: Total 171 (delta 21), reused 38 (delta 11), pack-reused 120\n",
            "Receiving objects: 100% (171/171), 142.23 MiB | 20.43 MiB/s, done.\n",
            "Resolving deltas: 100% (49/49), done.\n",
            "Checking out files: 100% (25/25), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7JDbG_psqr0",
        "colab_type": "code",
        "outputId": "869c9de5-d93e-4986-c5ab-8e130d9ae4cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZOl_GKLstcn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"../../gdrive/My Drive/reliable_news_prep\", \"r\") as reliable_file:\n",
        "    rel = [line.strip() for line in reliable_file]\n",
        "with open(\"../../gdrive/My Drive/fake_news_prep\", \"r\") as fake_file:\n",
        "    fake = [line.strip() for line in fake_file]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rXCS3m3x8VW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clean_fake = []\n",
        "for line in fake:\n",
        "    if (\"blockchain\" in line) and (\"bitcoin\" in line) and (\"headline\" in line):\n",
        "        line = line.split()\n",
        "        line = line[10:]\n",
        "        line = (\" \").join(line)\n",
        "    clean_fake.append(line)\n",
        "    \n",
        "fake = clean_fake"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lW8QDSRQx9Eg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uc9rEZ5iyG3U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4f2693af-1ac4-4c45-87ce-564b0b8d28d4"
      },
      "source": [
        "# The actual parameter optimalization\n",
        "dropout_rates = [0, 0.5]\n",
        "optimizers= ['rmsprop', 'adam']\n",
        "\n",
        "y_train = label_to_vector(Y_train)\n",
        "y_val = label_to_vector(Y_val)\n",
        "\n",
        "import keras\n",
        "counter = 0\n",
        "\n",
        "\n",
        "for rate in dropout_rates:\n",
        "  for optimizer in optimizers:\n",
        "    \n",
        "    print('a modell hiperparaméterei: ')\n",
        "    #print('dimension: {}'.format(dimension))\n",
        "    print('dropout rate: {}'.format(rate))\n",
        "    print('optimizer: {}'.format(optimizer))\n",
        "\n",
        "    model = bidirectional_LSTM(300, rate, 1100, 2, True, embedding_matrix)\n",
        "    model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
        "    callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)]\n",
        "\n",
        "    result = model.fit(x_train,\n",
        "              y_train,\n",
        "              epochs=10,\n",
        "              callbacks=callbacks,\n",
        "              validation_data=(x_val, y_val),\n",
        "              verbose=2,  # Logs once per epoch.\n",
        "              batch_size=128)\n",
        "\n",
        "    model.save_weights('../../gdrive/My Drive/sequence_model_with_pre_trained_embedding_{}.h5'.format(counter))\n",
        "    counter +=1\n",
        "\n"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a modell hiperparaméterei: \n",
            "dropout rate: 0\n",
            "optimizer: rmsprop\n",
            "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "Train on 27999 samples, validate on 8001 samples\n",
            "Epoch 1/10\n",
            " - 985s - loss: 0.2142 - acc: 0.9180 - val_loss: 0.1198 - val_acc: 0.9616\n",
            "Epoch 2/10\n",
            " - 979s - loss: 0.1115 - acc: 0.9610 - val_loss: 0.0983 - val_acc: 0.9695\n",
            "Epoch 3/10\n",
            " - 985s - loss: 0.0884 - acc: 0.9701 - val_loss: 0.0914 - val_acc: 0.9726\n",
            "Epoch 4/10\n",
            " - 979s - loss: 0.0762 - acc: 0.9743 - val_loss: 0.1044 - val_acc: 0.9689\n",
            "Epoch 5/10\n",
            " - 977s - loss: 0.0674 - acc: 0.9771 - val_loss: 0.0958 - val_acc: 0.9715\n",
            "Epoch 6/10\n",
            " - 967s - loss: 0.0574 - acc: 0.9803 - val_loss: 0.1063 - val_acc: 0.9634\n",
            "a modell hiperparaméterei: \n",
            "dropout rate: 0\n",
            "optimizer: adam\n",
            "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "Train on 27999 samples, validate on 8001 samples\n",
            "Epoch 1/10\n",
            " - 974s - loss: 0.2522 - acc: 0.8900 - val_loss: 0.1306 - val_acc: 0.9543\n",
            "Epoch 2/10\n",
            " - 968s - loss: 0.1020 - acc: 0.9639 - val_loss: 0.1436 - val_acc: 0.9530\n",
            "Epoch 3/10\n",
            " - 952s - loss: 0.0623 - acc: 0.9790 - val_loss: 0.1192 - val_acc: 0.9605\n",
            "Epoch 4/10\n",
            " - 942s - loss: 0.0415 - acc: 0.9861 - val_loss: 0.1145 - val_acc: 0.9689\n",
            "Epoch 5/10\n",
            " - 948s - loss: 0.0262 - acc: 0.9917 - val_loss: 0.1242 - val_acc: 0.9670\n",
            "Epoch 6/10\n",
            " - 942s - loss: 0.0162 - acc: 0.9951 - val_loss: 0.1610 - val_acc: 0.9646\n",
            "Epoch 7/10\n",
            " - 945s - loss: 0.0176 - acc: 0.9945 - val_loss: 0.1520 - val_acc: 0.9655\n",
            "a modell hiperparaméterei: \n",
            "dropout rate: 0.5\n",
            "optimizer: rmsprop\n",
            "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "Train on 27999 samples, validate on 8001 samples\n",
            "Epoch 1/10\n",
            " - 962s - loss: 0.2371 - acc: 0.9076 - val_loss: 0.1290 - val_acc: 0.9558\n",
            "Epoch 2/10\n",
            " - 961s - loss: 0.1170 - acc: 0.9594 - val_loss: 0.1148 - val_acc: 0.9590\n",
            "Epoch 3/10\n",
            " - 953s - loss: 0.0934 - acc: 0.9685 - val_loss: 0.0996 - val_acc: 0.9675\n",
            "Epoch 4/10\n",
            " - 947s - loss: 0.0776 - acc: 0.9739 - val_loss: 0.1042 - val_acc: 0.9656\n",
            "Epoch 5/10\n",
            " - 952s - loss: 0.0669 - acc: 0.9773 - val_loss: 0.1119 - val_acc: 0.9658\n",
            "Epoch 6/10\n",
            " - 950s - loss: 0.0628 - acc: 0.9780 - val_loss: 0.0862 - val_acc: 0.9668\n",
            "Epoch 7/10\n",
            " - 955s - loss: 0.0521 - acc: 0.9818 - val_loss: 0.0936 - val_acc: 0.9734\n",
            "Epoch 8/10\n",
            " - 950s - loss: 0.0494 - acc: 0.9836 - val_loss: 0.0847 - val_acc: 0.9723\n",
            "Epoch 9/10\n",
            " - 951s - loss: 0.0445 - acc: 0.9848 - val_loss: 0.0907 - val_acc: 0.9744\n",
            "Epoch 10/10\n",
            " - 953s - loss: 0.0404 - acc: 0.9865 - val_loss: 0.0902 - val_acc: 0.9754\n",
            "a modell hiperparaméterei: \n",
            "dropout rate: 0.5\n",
            "optimizer: adam\n",
            "Train on 27999 samples, validate on 8001 samples\n",
            "Epoch 1/10\n",
            " - 963s - loss: 0.2623 - acc: 0.8862 - val_loss: 0.1306 - val_acc: 0.9560\n",
            "Epoch 2/10\n",
            " - 961s - loss: 0.1024 - acc: 0.9646 - val_loss: 0.1108 - val_acc: 0.9644\n",
            "Epoch 3/10\n",
            " - 963s - loss: 0.0680 - acc: 0.9781 - val_loss: 0.1143 - val_acc: 0.9658\n",
            "Epoch 4/10\n",
            " - 963s - loss: 0.0478 - acc: 0.9853 - val_loss: 0.1326 - val_acc: 0.9664\n",
            "Epoch 5/10\n",
            " - 965s - loss: 0.0369 - acc: 0.9885 - val_loss: 0.1393 - val_acc: 0.9636\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNSwvHZ7zXbE",
        "colab_type": "text"
      },
      "source": [
        "# Final model training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xang5IIKNnd1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        },
        "outputId": "f37ccac0-64df-48b8-dabc-fe45494f062d"
      },
      "source": [
        "# Last training cycle after parameter optimalization, with all the data\n",
        "import keras\n",
        "\n",
        "model = bidirectional_LSTM(300, 0.5, 1100, 2, True, embedding_matrix)\n",
        "model.compile(optimizer='rmsprop', loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
        "\n",
        "callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)]\n",
        "\n",
        "model.fit(x_train,\n",
        "          y_train,\n",
        "          epochs=10,\n",
        "          callbacks=callbacks,\n",
        "          validation_data=(x_val, y_val),\n",
        "          verbose=2,  # Logs once per epoch.\n",
        "          batch_size=128)\n",
        "\n",
        "model.save_weights('../../gdrive/My Drive/final_sequence_model_with_pre_trained_embedding.h5')\n",
        "\n",
        "model.summary()\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "Train on 70000 samples, validate on 20000 samples\n",
            "Epoch 1/10\n",
            " - 2565s - loss: 0.2222 - acc: 0.9108 - val_loss: 0.1396 - val_acc: 0.9487\n",
            "Epoch 2/10\n",
            " - 2603s - loss: 0.1369 - acc: 0.9499 - val_loss: 0.1440 - val_acc: 0.9497\n",
            "Epoch 3/10\n",
            " - 2617s - loss: 0.1145 - acc: 0.9576 - val_loss: 0.1170 - val_acc: 0.9570\n",
            "Epoch 4/10\n",
            " - 2597s - loss: 0.1039 - acc: 0.9625 - val_loss: 0.1040 - val_acc: 0.9603\n",
            "Epoch 5/10\n",
            " - 2543s - loss: 0.0932 - acc: 0.9662 - val_loss: 0.1030 - val_acc: 0.9603\n",
            "Epoch 6/10\n",
            " - 2560s - loss: 0.0853 - acc: 0.9701 - val_loss: 0.0964 - val_acc: 0.9637\n",
            "Epoch 7/10\n",
            " - 2650s - loss: 0.0791 - acc: 0.9715 - val_loss: 0.0998 - val_acc: 0.9683\n",
            "Epoch 8/10\n",
            " - 2707s - loss: 0.0708 - acc: 0.9749 - val_loss: 0.0971 - val_acc: 0.9686\n",
            "Epoch 9/10\n",
            " - 2647s - loss: 0.0630 - acc: 0.9787 - val_loss: 0.0819 - val_acc: 0.9719\n",
            "Epoch 10/10\n",
            " - 2649s - loss: 0.0571 - acc: 0.9802 - val_loss: 0.0784 - val_acc: 0.9722\n",
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         (None, 1100)              0         \n",
            "_________________________________________________________________\n",
            "embedding_2 (Embedding)      (None, 1100, 300)         3000300   \n",
            "_________________________________________________________________\n",
            "spatial_dropout1d_2 (Spatial (None, 1100, 300)         0         \n",
            "_________________________________________________________________\n",
            "bidirectional_2 (Bidirection (None, 40)                51360     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 70)                2870      \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 70)                0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 2)                 142       \n",
            "=================================================================\n",
            "Total params: 3,054,672\n",
            "Trainable params: 3,054,672\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXlsi3HuNpZJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p26SeJMw5xR4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}